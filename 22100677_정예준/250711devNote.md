# 25.07.11 중간 정리

### 1. 데이터 준비 및 초기 특성 생성 파트

초기 단계에서는 원본 데이터를 로드하고 기본적인 결측치 처리를 진행했습니다. 데이터에서 어떤 컬럼을 취하고 버릴지를 많이 고민하였습니다.

- 원본 컬럼 처리 방식의 변화: Age의 결측치를 단순히 평균이나 중앙값으로 채우는 대신, Name에서 추출한 Title (호칭) 그룹별 중앙값을 사용하여 더 정확한 값을 추정하게 되었습니다. Embarked의 결측치도 단순 중앙값 대신 최빈값으로 처리하여 데이터의 일반적인 경향을 따랐습니다.

- 새로운 특성 생성의 시작: SibSp와 Parch를 조합하여 **FamilySize**라는 특성을 만들었습니다. 이는 가족 규모가 생존율에 미치는 영향을 파악하기 위함입니다.

### 2. 핵심 특성 엔지니어링 및 컬럼 선택 파트

- **Sex_Pclass** 추가: Sex와 Pclass라는 두 개의 강력한 원본 특성을 단순히 사용하는 것을 넘어, 이 둘을 문자열로 결합하여 **Sex_Pclass**라는 새로운 특성을 만들었습니다. 이 변화는 "여성이면서 1등석"과 같은 특정 그룹의 생존율이 매우 높다는 사실을 모델에 직접적으로 알려주어 예측력을 크게 향상시켰습니다. 그리고 각각의 원래 특성은 컬럼에서 제외하였습니다.

- **Deck**: 많은 결측치를 가진 Cabin 컬럼을 버리고, Cabin의 첫 글자를 추출하여 **Deck**이라는 새로운 범주형 특성을 만들었습니다. 이는 탑승 위치와 생존율의 상관관계를 반영하려는 시도였습니다.

- **Fare_Per_Person**: 전체 Fare 대신, FamilySize를 고려한 **Fare_Per_Person**을 도입하여 1인당 실제 지불한 요금을 반영하도록 변화했습니다.

- 범주형 인코딩 적용: 새롭게 생성되거나 유지하기로 결정된 범주형 특성들(Sex_Pclass, Deck, Embarked, Title)에 대해 LabelEncoder를 적용하여 모델이 학습할 수 있는 수치형 데이터로 변환했습니다.

초기에는 어떤 컬럼을 남겨야 할지 모호했지만 Age, FamilySize, Fare_Per_Person, Sex_Pclass, Deck, Embarked, Title 이렇게 7개의 핵심 컬럼만을 최종 모델에 사용하기로 결정했습니다. 이에 따라 PassengerId, Name, SibSp, Parch, Ticket, Sex, Pclass, Cabin, Fare와 같은 원본 컬럼들은 파생 특성 생성 후 최종적으로 제거하는 방식으로 전처리 하였습니다.

### 3. 하이퍼파라미터 튜닝 파트 (속도와 효율성)

모델의 성능을 최적화하기 위해 Optuna를 사용했지만, 튜닝 시간 단축을 위한 변화가 중요했습니다.

- 튜닝 속도 향상을 위해 각 Optuna 트라이얼에서 모델을 5번 학습/평가하던 것을 3번으로 줄여 전체 튜닝 시간을 크게 단축했습니다.

- 하이퍼파라미터 탐색 범위 조정: max_depth, n_estimators, learning_rate 등 주요 파라미터들의 suggest_int나 suggest_float 범위를 더 합리적이고 좁게 설정하여 탐색 공간을 줄였습니다.

- 조기 종료 라운드 단축: XGBoost와 LightGBM의 early_stopping_rounds를 50에서 30으로 줄여 더 빠르게 비효율적인 학습을 중단하도록 했습니다.

- 프루너(Pruner) 워밍업 단계 단축: Optuna의 MedianPruner가 가지치기를 시작하는 n_warmup_steps를 30에서 15로 줄여 비효율적인 트라이얼을 더 일찍 중단시켰습니다.

- **병렬 처리 도입**: n_jobs=-1 옵션을 추가하여 여러 CPU 코어를 활용하도록 하여 튜닝 속도를 비약적으로 향상시켰습니다. 이를 통해 튜닝 속도가 매우 향상되었습니다.

### 4. 최종 모델 구축 파트

- Optuna 튜닝 시 xgb.train에서 사용된 파라미터 이름과 xgb.XGBClassifier에서 사용되는 파라미터 이름의 불일치 때문에 오류가 생겼었고, 이를 해결했습니다.

- 경고 메시지 관리: xgboost의 "UserWarning: Parameters: { "verbose" } are not used." 경고는 xgb.XGBClassifier 모델 생성 시 verbose 파라미터가 무시되기 때문에 발생한다는 것을 인지하고, 해당 파라미터를 코드에서 제거하여 불필요한 경고를 없앴습니다.

### 추가적으로 깨달은 사항

- train 데이터를 분석하던 중, train 데이터에 있는 사람들 중 생존자 비율이 궁금해서 계산해보았고 38.38383838...% 인 것을 발견하였습니다. 이후 하이퍼파라미터로 튜닝이 완료된 결정 트리로 test 파일을 추론했을 때 생존자 비율이 얼마인지를 확인해보고 간접적으로 튜닝에 유의미한 변화가 있었는지 확인하였습니다.

- 튜토리얼 등 다양한 자료를 참고해보니 판단에 사용될 컬럼 수가 적음에도 불구하고 결정 트리가 잘 작동되는 것을 보았습니다.

- Optuna와 tqdm을 함께 사용해서 프로그래스바를 출력하여 실행 결과를 직관적이고 편하게 확인할 수 있었습니다.

- 파이썬의 수많은 라이브러리의 호환성을 맞추는 것이 쉽지 않다는 것을 깨달았습니다.

- 하이퍼파라미터 튜닝 파트를 보고 이 때 만큼은 사람보다 컴퓨터가 훨씬 고생한다는 것을 깨달았습니다.
