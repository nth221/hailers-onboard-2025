### 프로젝트 Ver2 시작, 변경점 정리

-   프로젝트 초기화 및 처음부터 다시 이 문제를 해결한다는 마음가짐으로 임함.
-   컬럼 선정은 적은 갯수부터 많은 갯수로 확장.
-   K-Fold 수를 5에서 6으로 변경했더니 최고점 갱신. 데이터의 컬럼 수가 적고, 복잡도가 낮은 모델이라서 분할 수가 1 커짐에도 불구하고 정확도가 더 높아진듯 하다.
-   optuna 사용은 아직 이르고, 지금 프로젝트에서 적절하지 못하다는 피드백을 반영하여 XgBoost와 랜덤 포레스트 두 가지 모델을 가지고 voting을 통한 앙상블 기법으로 하이퍼파라미터 튜닝을 하기로 했다.

#

### 정확도가 높았던 모델의 특징

-   XGBoost의 경우 depth가 낮은 편에 속했고, n_estimators는 80~400 정도에 머물렀다.
-   랜덤 포레스트의 경우 depth가 8~10 으로 깊은 편이고, n_estimators는 80~300 정도에 머물렀다.
-   개별 모델의 최고 교차 검증 정확도는 0.84 ~ 0.86 정도였지만 앙상블을 통해 모델을 통합하니 훈련 정확도가 0.92까지 높아진걸 확인할 수 있었다.
