## 07/10 (목)
---
**1. 튜토리얼 진행**
- 튜토리얼에서는 `RandomForest`를 사용함
- **`RandomForestClassifier`** 
    - 사용한 feature : `["Pclass", "Sex", "SibSp", "Parch"]`
    **=> 0.77511**

**2. `XGBoost` 사용**
- `RandomForest`와 `XGBoost` 둘 다 한 번 사용해보기로 함
    - `XGBoost` 스터디
    - **`XGBClassifier`**
        - 사용한 feature : `["Pclass", "Sex", "SibSp", "Parch"]`
        **=> 0.77272**

**3. feature 추가 시도**
- 성능을 높히기 위해 더 많은 feature를 추가해보기로 결정
    - `XGBoost`에 `["Age", "Fare"]` feature 추가
        **=> 0.77511**

**4. `XGBoost`의 random forest 사용**
- `XGBoost`에서도 RandomForest class가 있다는 사실을 알게되어 시도해봄
    - **`XGBRFClassifier`**
    **=> 0.78229**

**5. Feature Engineering**
- data에서 Feature Engineering이 필요함을 튜토리얼 영상에서 알게됨
    => 빈값을 채우거나 새로운 feature를 만드는 것을 시도함
- **`XGBRFClassifier`**에서 feature `["Pclass", "Sex"]`에 `["Age", "Fare_Per_Person", "Title", "Family_Size"]` 추가
    **=> 0.75837**
- 우선 age의 빈값을 채우기로 결정
- age를 어떤 기준으로 채울 것인지를 여러 방법을 통해 시도해보기로 회의함
- 각자 맡은 부분을 수행하고 정확도를 비교하기로 결정
---
## 최고 점수 : 0.78229
