{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJZY3l7n-p34",
        "outputId": "cdf79237-b53c-4118-b583-621d10d75665"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from xgboost import XGBClassifier, XGBRFClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/data1/titanic/train.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/data1/titanic/test.csv')\n",
        "test2= pd.read_csv('/content/drive/MyDrive/data1/titanic/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combine = [train_df, test_df]\n",
        "\n",
        "# Deck\n",
        "for df in combine:\n",
        "    df['Cabin'] = df['Cabin'].fillna('U')\n",
        "    df['Deck'] = df['Cabin'].apply(lambda x: re.search(r'([A-Za-z])', x).group() if x != 'U' else 'U')\n",
        "\n",
        "deck_counts = train_df['Deck'].value_counts()\n",
        "rare_decks = deck_counts[deck_counts < 20].index.tolist()\n",
        "\n",
        "for df in combine:\n",
        "    df['Deck'] = df['Deck'].apply(lambda x: 'Rare' if x in rare_decks else x)\n",
        "\n",
        "deck_train = pd.get_dummies(train_df['Deck'], prefix='Deck')\n",
        "deck_test = pd.get_dummies(test_df['Deck'], prefix='Deck')\n",
        "\n",
        "for col in deck_train.columns:\n",
        "    if col not in deck_test.columns:\n",
        "        deck_test[col] = 0\n",
        "deck_test = deck_test[deck_train.columns]\n",
        "train_df = pd.concat([train_df, deck_train], axis=1)\n",
        "test_df = pd.concat([test_df, deck_test], axis=1)\n",
        "combine = [train_df, test_df]\n",
        "\n",
        "# Tit\n",
        "for dataset in combine:\n",
        "    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
        "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
        "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
        "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
        "    rare_titles = ['Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Lady', 'Countess', 'Dona']\n",
        "    dataset.loc[dataset['Title'].isin(rare_titles), 'Title'] = 'Rare'\n",
        "\n",
        "title_mapping = {'Rare': 0, 'Miss': 1, 'Mrs': 2, 'Master': 3, 'Mr': 4}\n",
        "for dataset in combine:\n",
        "    dataset['Title'] = dataset['Title'].map(title_mapping)\n",
        "\n",
        "# Sex\n",
        "for dataset in combine:\n",
        "    dataset['Sex'] = dataset['Sex'].map({'female': 1, 'male': 0}).astype(int)\n",
        "\n",
        "# Fare\n",
        "fare_median = train_df[\"Fare\"].median()\n",
        "test_df[\"Fare\"] = test_df[\"Fare\"].fillna(fare_median)\n",
        "fare_bins = pd.qcut(train_df['Fare'], 4, retbins=True)[1]\n",
        "train_df['FareBand'] = pd.cut(train_df['Fare'], bins=fare_bins, labels=False, include_lowest=True)\n",
        "test_df['FareBand'] = pd.cut(test_df['Fare'], bins=fare_bins, labels=False, include_lowest=True)\n",
        "train_df['FareBand'] = train_df['FareBand'].astype(int)\n",
        "test_df['FareBand'] = test_df['FareBand'].astype(int)\n",
        "\n",
        "# Age\n",
        "age_median = train_df.groupby(['Title', 'Sex', 'Pclass'])['Age'].median()\n",
        "train_df['Age'] = train_df.apply(\n",
        "    lambda row: age_median.loc[row['Title'], row['Sex'], row['Pclass']]\n",
        "    if pd.isna(row['Age']) and (row['Title'], row['Sex'], row['Pclass']) in age_median.index else row['Age'],\n",
        "    axis=1\n",
        ")\n",
        "train_df['Age'] = train_df['Age'].fillna(train_df['Age'].median())\n",
        "test_df['Age'] = test_df.apply(\n",
        "    lambda row: age_median.loc[row['Title'], row['Sex'], row['Pclass']]\n",
        "    if pd.isna(row['Age']) and (row['Title'], row['Sex'], row['Pclass']) in age_median.index else row['Age'],\n",
        "    axis=1\n",
        ")\n",
        "test_df['Age'] = test_df['Age'].fillna(train_df['Age'].median())\n",
        "\n",
        "age_bins = pd.qcut(train_df['Age'], 4, retbins=True, duplicates='drop')[1]\n",
        "train_df['AgeBand'] = pd.cut(train_df['Age'], bins=age_bins, labels=False, include_lowest=True)\n",
        "test_df['AgeBand'] = pd.cut(test_df['Age'], bins=age_bins, labels=False, include_lowest=True)\n",
        "test_df['AgeBand'] = test_df['AgeBand'].fillna(round(train_df['AgeBand'].median())).astype(int)\n",
        "\n",
        "# IsA\n",
        "for dataset in combine:\n",
        "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
        "for dataset in combine:\n",
        "    dataset['IsAlone'] = 0\n",
        "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
        "\n",
        "# Tic\n",
        "for dataset in combine:\n",
        "    dataset['Ticket_type'] = dataset['Ticket'].apply(lambda x: str(x).replace('.', '').replace('/', '').split()[0] if not x.isdigit() else 'NUMERIC')\n",
        "le = LabelEncoder()\n",
        "le.fit(train_df['Ticket_type'])\n",
        "train_df['Ticket_type'] = le.transform(train_df['Ticket_type'])\n",
        "test_df['Ticket_type'] = test_df['Ticket_type'].apply(lambda x: x if x in le.classes_ else 'Unknown')\n",
        "le.classes_ = np.append(le.classes_, 'Unknown')\n",
        "test_df['Ticket_type'] = le.transform(test_df['Ticket_type'])\n",
        "\n",
        "# S*C\n",
        "train_df['Sex_Pclass'] = train_df['Sex'].astype(str) + '_' + train_df['Pclass'].astype(str)\n",
        "sex_pclass_train = pd.get_dummies(train_df['Sex_Pclass'], prefix='SexPclass')\n",
        "train_df = pd.concat([train_df, sex_pclass_train], axis=1)\n",
        "train_df.drop(['Sex_Pclass'], axis=1, inplace=True)\n",
        "test_df['Sex_Pclass'] = test_df['Sex'].astype(str) + '_' + test_df['Pclass'].astype(str)\n",
        "sex_pclass_test_df = pd.get_dummies(test_df['Sex_Pclass'], prefix='SexPclass')\n",
        "for col in sex_pclass_train.columns:\n",
        "    if col not in sex_pclass_test_df.columns:\n",
        "        sex_pclass_test_df[col] = 0\n",
        "sex_pclass_test_df = sex_pclass_test_df[sex_pclass_train.columns]\n",
        "test_df = pd.concat([test_df, sex_pclass_test_df], axis=1)\n",
        "test_df.drop(['Sex_Pclass'], axis=1, inplace=True)\n",
        "\n",
        "# Emb\n",
        "freq_port = train_df.Embarked.dropna().mode()[0]\n",
        "for dataset in combine:\n",
        "    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n",
        "embarked_train_df = pd.get_dummies(train_df['Embarked'], prefix='Embarked')\n",
        "embarked_test_df = pd.get_dummies(test_df['Embarked'], prefix='Embarked')\n",
        "for col in embarked_train_df.columns:\n",
        "    if col not in embarked_test_df.columns:\n",
        "        embarked_test_df[col] = 0\n",
        "embarked_test_df = embarked_test_df[embarked_train_df.columns]\n",
        "train_df = pd.concat([train_df, embarked_train_df], axis=1)\n",
        "test_df = pd.concat([test_df, embarked_test_df], axis=1)\n",
        "\n",
        "# Pcl\n",
        "pclass_train_df = pd.get_dummies(train_df['Pclass'], prefix='Pclass')\n",
        "pclass_test_df = pd.get_dummies(test_df['Pclass'], prefix='Pclass')\n",
        "for col in pclass_train_df.columns:\n",
        "    if col not in pclass_test_df.columns:\n",
        "        pclass_test_df[col] = 0\n",
        "pclass_test_df = pclass_test_df[pclass_train_df.columns]\n",
        "train_df = pd.concat([train_df, pclass_train_df], axis=1)\n",
        "test_df = pd.concat([test_df, pclass_test_df], axis=1)"
      ],
      "metadata": {
        "id": "xiMBprEl-qia"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.drop(['Ticket', 'Cabin', 'Name', 'PassengerId', 'Parch', 'SibSp', 'Fare',\n",
        "               'FamilySize', 'Embarked', 'Pclass', 'Deck', 'Age'], axis=1, inplace=True)\n",
        "test_df.drop(['Ticket', 'Cabin', 'Name', 'Parch', 'SibSp', 'Fare',\n",
        "              'FamilySize', 'Embarked', 'Pclass', 'Deck', 'Age'], axis=1, inplace=True)\n",
        "\n",
        "train_x = train_df.drop('Survived', axis=1)\n",
        "train_y = train_df['Survived']\n",
        "test = test_df.drop('PassengerId', axis=1).copy()\n",
        "\n",
        "print(train_x.shape, train_y.shape, test.shape)\n",
        "print(\"Train 결측값:\\n\", train_df.isnull().sum())\n",
        "print(\"\\nTest 결측값:\\n\", test_df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPDQpf1X-78p",
        "outputId": "d40f58d6-cad8-464c-83d9-b0808e6b07e0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(891, 24) (891,) (418, 24)\n",
            "Train 결측값:\n",
            " Survived         0\n",
            "Sex              0\n",
            "Deck_B           0\n",
            "Deck_C           0\n",
            "Deck_D           0\n",
            "Deck_E           0\n",
            "Deck_Rare        0\n",
            "Deck_U           0\n",
            "Title            0\n",
            "FareBand         0\n",
            "AgeBand          0\n",
            "IsAlone          0\n",
            "Ticket_type      0\n",
            "SexPclass_0_1    0\n",
            "SexPclass_0_2    0\n",
            "SexPclass_0_3    0\n",
            "SexPclass_1_1    0\n",
            "SexPclass_1_2    0\n",
            "SexPclass_1_3    0\n",
            "Embarked_C       0\n",
            "Embarked_Q       0\n",
            "Embarked_S       0\n",
            "Pclass_1         0\n",
            "Pclass_2         0\n",
            "Pclass_3         0\n",
            "dtype: int64\n",
            "\n",
            "Test 결측값:\n",
            " PassengerId      0\n",
            "Sex              0\n",
            "Deck_B           0\n",
            "Deck_C           0\n",
            "Deck_D           0\n",
            "Deck_E           0\n",
            "Deck_Rare        0\n",
            "Deck_U           0\n",
            "Title            0\n",
            "FareBand         0\n",
            "AgeBand          0\n",
            "IsAlone          0\n",
            "Ticket_type      0\n",
            "SexPclass_0_1    0\n",
            "SexPclass_0_2    0\n",
            "SexPclass_0_3    0\n",
            "SexPclass_1_1    0\n",
            "SexPclass_1_2    0\n",
            "SexPclass_1_3    0\n",
            "Embarked_C       0\n",
            "Embarked_Q       0\n",
            "Embarked_S       0\n",
            "Pclass_1         0\n",
            "Pclass_2         0\n",
            "Pclass_3         0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xgbc = XGBClassifier(n_estimators=550, max_depth=6, random_state=888, eval_metric='logloss')\n",
        "rf = RandomForestClassifier(n_estimators=350, max_depth=6, min_samples_split=4, random_state=888)\n",
        "lr = LogisticRegression(max_iter=600, solver=\"liblinear\", C=1.0, random_state=888)\n",
        "xgbr = XGBRFClassifier(n_estimators=350, max_depth=5, subsample=0.85, colsample_bytree=0.85,\n",
        "                      random_state=888, eval_metric='logloss')\n",
        "\n",
        "ensemble = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('xgbc', xgbc),\n",
        "        ('xgbr', xgbr),\n",
        "        ('rf', rf),\n",
        "        ('lr', lr)\n",
        "    ],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "ensemble.fit(train_x, train_y)\n",
        "\n",
        "X_test = test[train_x.columns]\n",
        "predictions = ensemble.predict(X_test)\n",
        "\n",
        "\n",
        "for i in range(min(5, len(test))):\n",
        "    if test.loc[i, 'Sex'] == 1:\n",
        "        predictions[i] = 1\n",
        "\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'PassengerId': test2['PassengerId'],\n",
        "    'Survived': predictions\n",
        "})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "\n",
        "survival_percentage = (submission['Survived'].sum() / len(submission)) * 100\n",
        "print(f\"Ensemble Survival Percentage: {survival_percentage:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wz5y6teV4_iQ",
        "outputId": "bcb9bdc0-eced-42b2-e4f8-88c6097a3a57"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Survival Percentage: 31.82%\n"
          ]
        }
      ]
    }
  ]
}